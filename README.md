# S3 Orchestrator

An S3-compatible orchestrator that combines multiple storage backends into a single unified endpoint. Add as many S3-compatible backends as you want — OCI Object Storage, Backblaze B2, AWS S3, MinIO, whatever — and the orchestrator presents them to clients as one or more virtual buckets. Per-backend quota enforcement lets you cap each backend at exactly the byte limit you choose, so you can stack multiple free-tier allocations from different providers into a single, larger storage target for backups, media, or anything else without worrying about surprise bills.

Multiple virtual buckets let different applications share the same orchestrator with isolated file namespaces and independent credentials. Each bucket's objects are stored with an internal key prefix (`{bucket}/{key}`), so bucket isolation requires zero changes to the storage layer or database schema.

Built-in cross-backend replication also makes this an easy way to keep your data in multiple clouds without touching your application. Point your app at the proxy, set a replication factor, and every object automatically lands in two or more providers — instant multi-cloud redundancy with zero client-side changes.

Objects are automatically routed to the first backend with available quota. Metadata and quota tracking live in PostgreSQL; the backends only see standard S3 API calls. The orchestrator is fully S3-compatible and works with any standard S3 client.

## Architecture

```
              S3 clients (aws cli, rclone, etc.)
                          |
                          v
                    +-----------+
                    | S3 Orch.  |  <-- SigV4 auth, rate limiting, quota routing
                    +-----------+
                     |         |
            +--------+         +------------------+------------------+
            v                  v                  v                  v
       PostgreSQL        OCI Object         Backblaze B2          AWS S3
       (metadata)       Storage (20 GB)       (10 GB)             (5 GB)
                              \                  |                  /
                               '------------ 35 GB total ---------'
```

- **PostgreSQL** stores object locations (`object_locations`), per-backend quota counters (`backend_quotas`), and multipart upload state (`multipart_uploads`, `multipart_parts`). Schema is applied automatically on startup via embedded SQL. All queries are generated by [sqlc](https://sqlc.dev/) from annotated SQL files and executed via [pgx/v5](https://github.com/jackc/pgx) connection pools.
- **Backends** are standard S3-compatible services accessed via AWS SDK v2. Any provider that speaks the S3 API works — OCI Object Storage, Backblaze B2, AWS S3, MinIO, Wasabi, etc.
- **Quota routing** selects the first backend with available space when writing objects. Quota is updated atomically in a transaction alongside the object location record. Set `quota_bytes: 0` (or omit it) to disable quota enforcement on a backend — useful when you don't need cost control and just want unified access or replication.
- **Usage limits** optionally cap monthly API requests, egress bytes, and ingress bytes per backend. When a backend exceeds a limit, writes overflow to other backends and reads fail over to replicas. Delete and abort operations always bypass limits. Limits are enforced using cached database totals (refreshed every 30s) plus unflushed in-memory counters.

## S3 API Coverage

| Operation | Method | Path | Notes |
|-----------|--------|------|-------|
| PutObject | `PUT` | `/{bucket}/{key}` | |
| GetObject | `GET` | `/{bucket}/{key}` | Supports `Range` header (206 Partial Content) |
| HeadObject | `HEAD` | `/{bucket}/{key}` | |
| DeleteObject | `DELETE` | `/{bucket}/{key}` | Idempotent (404 from store treated as success) |
| CopyObject | `PUT` | `/{bucket}/{key}` | Uses `X-Amz-Copy-Source` header |
| ListObjectsV2 | `GET` | `/{bucket}?list-type=2` | Supports `delimiter` for virtual directories |
| CreateMultipartUpload | `POST` | `/{bucket}/{key}?uploads` | |
| UploadPart | `PUT` | `/{bucket}/{key}?partNumber=N&uploadId=X` | |
| CompleteMultipartUpload | `POST` | `/{bucket}/{key}?uploadId=X` | |
| AbortMultipartUpload | `DELETE` | `/{bucket}/{key}?uploadId=X` | |
| ListParts | `GET` | `/{bucket}/{key}?uploadId=X` | |

Each request must target a virtual bucket name that matches the credentials used to sign the request. Requests to a bucket the credentials aren't authorized for return `403 AccessDenied`.

## Authentication & Multi-Bucket

Each virtual bucket has one or more credential sets. On every request, the orchestrator:

1. Extracts the access key from the SigV4 `Authorization` header (or token from `X-Proxy-Token`).
2. Looks up which bucket the credential belongs to.
3. Verifies the signature (SigV4) or token.
4. Validates the URL path bucket matches the authorized bucket.

Two auth methods are supported, checked in order:

1. **AWS SigV4** (recommended) - Standard AWS Signature Version 4 via the `Authorization` header. Compatible with `aws cli`, SDKs, and any S3 client.
2. **Legacy token** - Simple `X-Proxy-Token` header for backward compatibility.

Multiple services can share a bucket by each having their own credentials that all map to the same bucket name. Access key IDs must be globally unique across all buckets.

Authentication is always required — every bucket must have at least one credential set.

For client usage examples (AWS CLI, rclone, boto3, Go SDK), see the [User Guide](docs/user-guide.md).
For deployment and operations, see the [Admin Guide](docs/admin-guide.md).

## Degraded Mode (Circuit Breaker)

A three-state circuit breaker wraps all database access:

```
closed (healthy) → open (DB down) → half-open (probing) → closed
```

When the database becomes unreachable (consecutive failures exceed `failure_threshold`), the orchestrator enters **degraded mode**:

- **Reads** broadcast to all backends in order. A location cache (TTL configurable via `cache_ttl`) stores successful lookups to avoid repeated broadcasts for the same key.
- **Writes** (PUT, DELETE, COPY, multipart) return `503 ServiceUnavailable`.
- **Health endpoint** returns `degraded` instead of `ok`.

After `open_timeout` elapses, the circuit enters half-open state and sends a single probe request. If the database responds, the circuit closes and normal operation resumes automatically.

## Rebalancing

The rebalancer periodically moves objects between backends to optimize storage distribution. Disabled by default to avoid unexpected egress charges.

Two strategies:

- **pack** - Fills backends in configuration order, consolidating free space on the last backend. Good for maximizing usable capacity on free-tier providers.
- **spread** - Equalizes utilization ratios across all backends. Good for distributing load evenly.

The `threshold` parameter (0–1) sets the minimum utilization spread required to trigger a rebalance run. Objects are moved in configurable batch sizes.

## Replication

When `replication.factor` is greater than 1, a background worker creates additional copies of objects on different backends to reach the target factor. Read operations automatically fail over to replicas if the primary copy is unavailable.

The worker runs once at startup to catch up on pending replicas, then continues at the configured interval.

## Rate Limiting

Optional per-IP token bucket rate limiting. When enabled, requests exceeding the configured rate return `429 SlowDown`. Stale IP entries are cleaned up automatically.

## Usage Limits

Per-backend monthly limits for API requests, egress bytes, and ingress bytes. Set any limit to `0` (or omit it) for unlimited. Limits reset naturally each month — the usage tracking table is keyed by `YYYY-MM` period.

**Enforcement behavior:**

- **Writes** (PutObject, CopyObject, CreateMultipartUpload, UploadPart) — backends over their limits are excluded from selection; writes overflow to the next eligible backend. If all backends are over-limit, the orchestrator returns `507 InsufficientStorage`.
- **Reads** (GetObject, HeadObject) — over-limit backends are skipped; the orchestrator tries replicas. Returns `429 SlowDown` only when *all* copies of the object are on over-limit backends.
- **Deletes** (DeleteObject, AbortMultipartUpload) — always allowed regardless of limits.

Effective usage is computed as `DB baseline + unflushed in-memory counters + proposed operation`, so enforcement stays accurate between the 30-second flush/refresh cycles without double-counting.

## Configuration

YAML config file specified via `-config` flag (default: `config.yaml`). Supports `${ENV_VAR}` expansion.

```yaml
server:
  listen_addr: "0.0.0.0:9000"
  max_object_size: 5368709120  # 5 GB (default)

# Virtual buckets with per-bucket credentials
buckets:
  - name: "app1-files"
    credentials:
      - access_key_id: "APP1_ACCESS_KEY"
        secret_access_key: "APP1_SECRET_KEY"

  - name: "shared-files"
    credentials:
      # Multiple services can share a bucket with separate credentials
      - access_key_id: "WRITER_ACCESS_KEY"
        secret_access_key: "WRITER_SECRET_KEY"
      - access_key_id: "READER_ACCESS_KEY"
        secret_access_key: "READER_SECRET_KEY"

  # Legacy token auth (backward compatibility)
  # - name: "legacy-bucket"
  #   credentials:
  #     - token: "my-secret-token"

database:
  host: "localhost"
  port: 5432
  database: "s3proxy"
  user: "s3proxy"
  password: "secret"
  ssl_mode: "require"
  max_conns: 10
  min_conns: 5
  max_conn_lifetime: "5m"

backends:
  - name: "oci"
    endpoint: "https://namespace.compat.objectstorage.region.oraclecloud.com"
    region: "us-phoenix-1"
    bucket: "my-bucket"
    access_key_id: "backend-access-key"
    secret_access_key: "backend-secret-key"
    force_path_style: true
    quota_bytes: 21474836480  # 20 GB (0 or omit for unlimited)
    api_request_limit: 0      # monthly API request limit (0 = unlimited)
    egress_byte_limit: 0      # monthly egress byte limit (0 = unlimited)
    ingress_byte_limit: 0     # monthly ingress byte limit (0 = unlimited)

telemetry:
  metrics:
    enabled: true
    path: "/metrics"
  tracing:
    enabled: true
    endpoint: "localhost:4317"
    insecure: true
    sample_rate: 1.0

circuit_breaker:
  failure_threshold: 3     # consecutive DB failures before opening (default: 3)
  open_timeout: "15s"      # delay before probing recovery (default: 15s)
  cache_ttl: "60s"         # key→backend cache TTL during degraded reads (default: 60s)

rebalance:
  enabled: false
  strategy: "pack"         # "pack" or "spread" (default: pack)
  interval: "6h"           # run interval (default: 6h)
  batch_size: 100          # max objects per run (default: 100)
  threshold: 0.1           # min utilization spread to trigger (default: 0.1)

replication:
  factor: 1                # copies per object; 1 = no replication (default: 1)
  worker_interval: "5m"    # replication worker cycle (default: 5m)
  batch_size: 50           # objects per cycle (default: 50)

rate_limit:
  enabled: false
  requests_per_sec: 100    # token refill rate (default: 100)
  burst: 200               # max burst size (default: 200)
```

## Database

The orchestrator connects to PostgreSQL via pgx/v5 connection pools and auto-applies its schema on startup (all DDL uses `IF NOT EXISTS`). Four tables are created:

| Table | Purpose |
|-------|---------|
| `backend_quotas` | Per-backend byte limits and usage counters |
| `object_locations` | Maps object keys to backends with size tracking |
| `multipart_uploads` | In-progress multipart upload metadata |
| `multipart_parts` | Individual parts for active multipart uploads |
| `usage_deltas` | Monthly per-backend API request and data transfer counters |

Quota updates are transactional: object location inserts/deletes and quota counter changes happen atomically.

All SQL queries live in `internal/storage/sqlc/queries/` as annotated `.sql` files. Type-safe Go code is generated by sqlc into `internal/storage/sqlc/`. To regenerate after editing queries:

```bash
make generate
```

## Telemetry

### Prometheus Metrics

All metrics are prefixed with `s3proxy_`. Exposed at `/metrics` when enabled.

| Metric | Type | Labels | Description |
|--------|------|--------|-------------|
| `s3proxy_build_info` | Gauge | version, go_version | Build metadata |
| `s3proxy_requests_total` | Counter | method, status_code | HTTP request count |
| `s3proxy_request_duration_seconds` | Histogram | method | Request latency |
| `s3proxy_request_size_bytes` | Histogram | method | Upload sizes |
| `s3proxy_response_size_bytes` | Histogram | method | Download sizes |
| `s3proxy_inflight_requests` | Gauge | method | Currently processing |
| `s3proxy_backend_requests_total` | Counter | operation, backend, status | Backend S3 API calls |
| `s3proxy_backend_duration_seconds` | Histogram | operation, backend | Backend latency |
| `s3proxy_manager_requests_total` | Counter | operation, backend, status | Manager-level operations |
| `s3proxy_manager_duration_seconds` | Histogram | operation, backend | Manager latency |
| `s3proxy_quota_bytes_used` | Gauge | backend | Current bytes used |
| `s3proxy_quota_bytes_limit` | Gauge | backend | Quota limit |
| `s3proxy_quota_bytes_available` | Gauge | backend | Remaining space |
| `s3proxy_objects_count` | Gauge | backend | Stored object count |
| `s3proxy_active_multipart_uploads` | Gauge | backend | In-progress uploads |
| `s3proxy_rebalance_objects_moved_total` | Counter | strategy, status | Objects moved by rebalancer |
| `s3proxy_rebalance_bytes_moved_total` | Counter | strategy | Bytes moved by rebalancer |
| `s3proxy_rebalance_runs_total` | Counter | strategy, status | Rebalancer executions |
| `s3proxy_rebalance_duration_seconds` | Histogram | strategy | Rebalancer execution time |
| `s3proxy_rebalance_skipped_total` | Counter | reason | Rebalancer runs skipped |
| `s3proxy_replication_pending` | Gauge | — | Objects below replication factor |
| `s3proxy_replication_copies_created_total` | Counter | — | Replica copies created |
| `s3proxy_replication_errors_total` | Counter | — | Replication errors |
| `s3proxy_replication_duration_seconds` | Histogram | — | Replication cycle time |
| `s3proxy_replication_runs_total` | Counter | status | Replication worker executions |
| `s3proxy_circuit_breaker_state` | Gauge | — | 0=closed, 1=open, 2=half-open |
| `s3proxy_circuit_breaker_transitions_total` | Counter | from, to | State transitions |
| `s3proxy_degraded_reads_total` | Counter | operation | Broadcast reads in degraded mode |
| `s3proxy_degraded_cache_hits_total` | Counter | — | Cache hits during degraded reads |
| `s3proxy_degraded_write_rejections_total` | Counter | operation | Writes rejected in degraded mode |
| `s3proxy_usage_api_requests` | Gauge | backend | Current month API request count |
| `s3proxy_usage_egress_bytes` | Gauge | backend | Current month egress bytes |
| `s3proxy_usage_ingress_bytes` | Gauge | backend | Current month ingress bytes |
| `s3proxy_usage_limit_rejections_total` | Counter | operation, limit_type | Operations rejected by usage limits |

Quota metrics are refreshed from PostgreSQL every 30 seconds (no backend API calls).

### OpenTelemetry Tracing

Spans are emitted for every HTTP request, manager operation, and backend S3 call. Traces propagate via W3C `traceparent` headers. Configured to export via gRPC OTLP to Tempo.

## Endpoints

| Path | Purpose |
|------|---------|
| `/health` | Health check — returns `ok` or `degraded` (always 200) |
| `/metrics` | Prometheus metrics |
| `/{bucket}/{key}` | S3 API |

## Background Tasks

| Task | Interval | Description |
|------|----------|-------------|
| Usage flush + metrics | 30s | Flushes in-memory usage counters to PostgreSQL, then refreshes quota stats, usage baselines, object counts, and multipart counts. Updates Prometheus gauges. |
| Stale multipart cleanup | 1h | Aborts multipart uploads older than 24h and deletes their temporary part objects. |
| Rebalancer | configurable (default 6h) | Moves objects between backends per strategy. Only runs when enabled. |
| Replicator | configurable (default 5m) | Creates copies of under-replicated objects. Only runs when factor > 1. Runs once at startup. |

## Sync Subcommand

Imports pre-existing objects from a backend S3 bucket into the orchestrator's metadata database. Useful when bringing an existing bucket under orchestrator management. The `--bucket` flag specifies which virtual bucket the imported objects belong to — keys are stored with a `{bucket}/` prefix for namespace isolation.

```bash
# Import all objects from a backend into the "unified" virtual bucket
s3-orchestrator sync --config config.yaml --backend oci --bucket unified

# Preview what would be imported
s3-orchestrator sync --config config.yaml --backend oci --bucket unified --dry-run

# Import only objects under a prefix
s3-orchestrator sync --config config.yaml --backend oci --bucket unified --prefix photos/
```

| Flag | Default | Description |
|------|---------|-------------|
| `--config` | `config.yaml` | Path to configuration file |
| `--backend` | (required) | Backend name to sync |
| `--bucket` | (required) | Virtual bucket name to prefix imported keys with |
| `--prefix` | `""` | Only sync objects with this key prefix |
| `--dry-run` | `false` | Preview what would be imported without writing |

Objects already tracked in the database for that backend are skipped. The command logs per-page progress and a final summary with imported count, skipped count, and total bytes imported.

## Development

```bash
# Regenerate sqlc query code (after editing .sql files)
make generate

# Run locally (requires PostgreSQL and config.yaml)
make run

# Lint
make lint

# Run unit tests
make test

# Run integration tests (requires Docker)
make integration-test

# Build local Docker image
make build

# Build multi-arch and push to registry
make push
```

## Deployment

Build and push a Docker image:

```bash
make push
```

### Prerequisites

- PostgreSQL database (schema auto-applied on startup)
- At least one S3-compatible storage backend
- Configuration file with credentials (see `config.example.yaml`)

## Project Structure

```
cmd/s3-orchestrator/
  main.go                    Entry point, subcommand dispatch, background tasks
  sync.go                    Sync subcommand (bucket import)
internal/
  auth/auth.go               BucketRegistry, SigV4 verification, legacy token auth
  config/config.go           YAML config loader with env var expansion
  server/
    server.go                HTTP router, bucket resolution, key prefixing, metrics
    objects.go               PUT, GET, HEAD, DELETE, COPY handlers
    list.go                  ListObjectsV2 handler (XML response)
    multipart.go             Multipart upload handlers
    helpers.go               Path parsing, S3 XML error responses
    ratelimit.go             Per-IP token bucket rate limiter
  storage/
    backend.go               S3 client (AWS SDK v2)
    metadata.go              MetadataStore interface, sentinel errors
    store.go                 PostgreSQL storage layer (pgx/v5 + sqlc)
    circuitbreaker.go        Three-state circuit breaker wrapper
    manager.go               Multi-backend routing, quota selection, cache
    manager_objects.go       Object CRUD with read failover + broadcast
    manager_multipart.go     Multipart upload lifecycle
    manager_usage.go         Usage tracking flush + period helpers
    manager_metrics.go       Quota metric recording
    rebalancer.go            Object rebalancing across backends
    replicator.go            Cross-backend object replication
    sqlc/
      schema.sql             Schema for sqlc code generation
      queries/               Annotated SQL query files
      *.go                   Generated type-safe query code (do not edit)
  telemetry/
    metrics.go               Prometheus metric definitions
    tracing.go               OpenTelemetry tracer setup
sqlc.yaml                    sqlc configuration
Dockerfile                   Multi-stage build
Makefile                     Build, test, lint, generate, push targets
config.example.yaml          Configuration reference
```
